import streamlit as st
import google.generativeai as genai
import os

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ (Page Config) ---
st.set_page_config(page_title="‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ - KU Sriracha Bot", page_icon="üêØ", layout="wide")

# --- 2. CSS ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á UI ‡πÉ‡∏´‡∏°‡πà (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≤‡∏ß) ---
st.markdown("""
<style>
    /* ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏´‡∏•‡∏±‡∏Å */
    .stApp { background-color: #FFFFFF; color: black; }
    
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Sidebar */
    [data-testid="stSidebar"] { 
        background-color: #00594C !important; 
    }
    
    /* ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏™‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô Sidebar ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô */
    [data-testid="stSidebar"] .stMarkdown p, 
    [data-testid="stSidebar"] h3, 
    [data-testid="stSidebar"] span,
    [data-testid="stSidebar"] label { 
        color: #FFFFFF !important; /* ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß */
    }

    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Expander ‡πÅ‡∏•‡∏∞ Input ‡πÉ‡∏ô Sidebar ‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡∏î‡∏≥ */
    .stSidebar .st-emotion-cache-1f3w0ih p, 
    .stSidebar .st-emotion-cache-1629ce8 p,
    .stSidebar input {
        color: #000000 !important;
    }

    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏∏‡πà‡∏°‡πÉ‡∏ô Sidebar */
    .stSidebar .stButton>button {
        background-color: #FFFFFF !important;
        color: #00594C !important;
        border-radius: 10px;
        border: none;
        font-weight: bold;
        width: 100%;
    }
    
    /* ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ Chat */
    h1 { color: #00594C !important; }
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; border: 1px solid #e0e0e0; }
</style>
""", unsafe_allow_html=True)

# --- 3. ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ API ‡πÅ‡∏•‡∏∞ Dynamic Model Selection ---
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Streamlit Secrets")
    st.stop()

genai.configure(api_key=api_key)

@st.cache_resource
def load_smart_model():
    try:
        available_models = [
            m.name for m in genai.list_models() 
            if 'generateContent' in m.supported_generation_methods
        ]
        selected_model = next((m for m in available_models if "1.5-flash" in m), available_models[0])
        
        instruction = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ' AI ‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà ‡∏°‡∏Å. ‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤ (KU SRC) "
            "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á' ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
            "1. ‡∏´‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ "
            "2. ‡∏´‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå Google Maps ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ "
            "3. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏•‡∏∞‡∏ï‡∏¥‡∏à‡∏π‡∏î/‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏à‡∏π‡∏î (GPS) ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î "
            "4. ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° ‡∏û‡∏µ‡πà-‡∏ô‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û"
        )
        return genai.GenerativeModel(model_name=selected_model, system_instruction=instruction)
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

model = load_smart_model()

# --- 4. ‡∏™‡πà‡∏ß‡∏ô Sidebar: Student Dashboard ---
with st.sidebar:
    st.image("https://www.src.ku.ac.th/th/images/logo/KU_Sriracha_Logo.png", width=150)
    st.markdown("### üéì Student Dashboard")
    
    # ‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡πà‡∏ß‡∏ô
    with st.expander("üìÑ ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡πà‡∏ß‡∏ô (‡∏Ñ‡∏•‡∏¥‡∏Å)"):
        st.link_button("üìù ‡πÉ‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°-‡∏ñ‡∏≠‡∏ô (KU3)", "https://registrar.ku.ac.th/wp-content/uploads/2023/11/KU3-Add-Drop-Form.pdf")
        st.link_button("üí∞ ‡πÉ‡∏ö‡∏ú‡πà‡∏≠‡∏ô‡∏ú‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°", "https://registrar.ku.ac.th/wp-content/uploads/2024/11/Postpone-tuition-and-fee-payments.pdf")
        st.link_button("üìÅ ‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ß‡∏°‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°", "https://reg2.src.ku.ac.th/download.html")

    st.markdown("---")
    
    # GPA Simulator
    st.markdown("### üî¢ GPA Simulator")
    # ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Container ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏µ
    with st.container():
        current_gpa = st.number_input("‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô", min_value=0.0, max_value=4.0, value=3.00, step=0.01)
        target_gpa = st.number_input("‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á", min_value=0.0, max_value=4.0, value=3.50, step=0.01)
        
        if st.button("‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÇ‡∏≠‡∏Å‡∏≤‡∏™"):
            if target_gpa > current_gpa:
                st.write(f"üì¢ ‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì {target_gpa} ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≠‡∏á!")
            else:
                st.balloons()
                st.write("üéâ ‡πÄ‡∏Å‡∏£‡∏î‡∏ô‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö!")

    st.markdown("---")
    st.caption("üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏≠‡∏á‡∏ñ‡∏≤‡∏°‡∏û‡∏µ‡πà‡∏ß‡πà‡∏≤ '‡∏ï‡∏∂‡∏Å 17 ‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏´‡∏ô' ‡∏î‡∏π‡∏™‡∏¥!")

# --- 5. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Chat History ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if os.path.exists("ku_data.txt"):
    with open("ku_data.txt", "r", encoding="utf-8") as f:
        knowledge_base = f.read()
else:
    knowledge_base = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏°‡∏Å. ‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤"

for message in st.session_state.messages:
    avatar = "üßë‚Äçüéì" if message["role"] == "user" else "üêØ"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- 6. ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ChatBot ---
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..."):
    st.chat_message("user", avatar="üßë‚Äçüéì").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant", avatar="üêØ"):
        placeholder = st.empty()
        placeholder.markdown("*(‡∏û‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏±‡∏ö...)*")
        
        history = [{"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]} 
                   for m in st.session_state.messages[-6:-1]]
        
        try:
            chat_session = model.start_chat(history=history)
            full_context = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{knowledge_base}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {prompt}"
            
            response = chat_session.send_message(full_context, stream=True)
            full_response = ""
            for chunk in response:
                full_response += chunk.text
                placeholder.markdown(full_response + "‚ñå")
            
            placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            st.rerun()
            
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
