import streamlit as st
import google.generativeai as genai
import os

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ (Page Config) ---
st.set_page_config(page_title="‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ - KU Sriracha Bot", page_icon="üêØ", layout="wide")

# --- 2. CSS ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á UI ‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏ó‡∏±‡∏ô‡∏™‡∏°‡∏±‡∏¢ (Custom Sidebar & Chat) ---
st.markdown("""
<style>
    .stApp { background-color: #FFFFFF; color: black; }
    [data-testid="stSidebar"] { background-color: #00594C !important; }
    .stSidebar [data-testid="stMarkdownContainer"] p, .stSidebar h3, .stSidebar span { color: white !important; font-weight: bold; }
    h1 { color: #00594C !important; font-family: 'Tahoma'; }
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; border: 1px solid #e0e0e0; }
    
    /* ‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏õ‡∏∏‡πà‡∏°‡πÉ‡∏ô Sidebar */
    .stButton>button { width: 100%; border-radius: 10px; border: none; background-color: #ffffff; color: #00594C; font-weight: bold; }
    .stButton>button:hover { background-color: #e0e0e0; color: #00594C; }
</style>
""", unsafe_allow_html=True)

# --- 3. ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ API ‡πÅ‡∏•‡∏∞ Dynamic Model Selection ---
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Streamlit Secrets")
    st.stop()

genai.configure(api_key=api_key)

@st.cache_resource
def load_smart_model():
    try:
        # ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô List Model ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        available_models = [
            m.name for m in genai.list_models() 
            if 'generateContent' in m.supported_generation_methods
        ]
        selected_model = next((m for m in available_models if "1.5-flash" in m), available_models[0])
        
        instruction = (
            "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ' AI ‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà ‡∏°‡∏Å. ‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤ (KU SRC) "
            "‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á' ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô "
            "1. ‡∏´‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ "
            "2. ‡∏´‡∏≤‡∏Å‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå Google Maps ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ "
            "3. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏•‡∏∞‡∏ï‡∏¥‡∏à‡∏π‡∏î/‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏à‡∏π‡∏î (GPS) ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡πá‡∏ô‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î "
            "4. ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° ‡∏û‡∏µ‡πà-‡∏ô‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û"
        )
        return genai.GenerativeModel(model_name=selected_model, system_instruction=instruction)
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None

model = load_smart_model()

# --- 4. ‡∏™‡πà‡∏ß‡∏ô Sidebar: Student Dashboard (‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà) ---
with st.sidebar:
    st.image("https://www.src.ku.ac.th/th/images/logo/KU_Sriracha_Logo.png", width=150)
    st.markdown("### üéì Student Dashboard")
    
    # Quick Links (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å ku_data.txt ‡∏°‡∏≤‡∏ó‡∏≥‡∏õ‡∏∏‡πà‡∏°‡∏•‡∏±‡∏î)
    with st.expander("üìÑ ‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏î‡πà‡∏ß‡∏ô"):
        st.link_button("üìù ‡πÉ‡∏ö‡πÄ‡∏û‡∏¥‡πà‡∏°-‡∏ñ‡∏≠‡∏ô (KU3)", "https://registrar.ku.ac.th/wp-content/uploads/2023/11/KU3-Add-Drop-Form.pdf")
        st.link_button("üí∞ ‡πÉ‡∏ö‡∏ú‡πà‡∏≠‡∏ô‡∏ú‡∏±‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡∏≠‡∏°", "https://registrar.ku.ac.th/wp-content/uploads/2024/11/Postpone-tuition-and-fee-payments.pdf")
        st.link_button("üìÅ ‡∏´‡∏ô‡πâ‡∏≤‡∏£‡∏ß‡∏°‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°", "https://reg2.src.ku.ac.th/download.html")

    st.markdown("---")
    
    # GPA Simulator (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πâ‡∏≤‡∏ß)
    st.markdown("### üî¢ GPA Simulator")
    current_gpa = st.number_input("‡πÄ‡∏Å‡∏£‡∏î‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏™‡∏∞‡∏™‡∏°‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô", min_value=0.0, max_value=4.0, value=3.00, step=0.01)
    target_gpa = st.number_input("‡πÄ‡∏Å‡∏£‡∏î‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏ó‡∏≠‡∏°‡∏ô‡∏µ‡πâ", min_value=0.0, max_value=4.0, value=3.50, step=0.01)
    
    if st.button("‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÇ‡∏≠‡∏Å‡∏≤‡∏™"):
        if target_gpa > current_gpa:
            st.warning(f"‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏¢‡∏±‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏ô‡∏∞! ‡∏ô‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ {target_gpa} ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏Å‡∏£‡∏î‡∏£‡∏ß‡∏°‡∏Ñ‡∏£‡∏±‡∏ö")
        else:
            st.balloons()
            st.success("‡∏¢‡∏≠‡∏î‡πÄ‡∏¢‡∏µ‡πà‡∏¢‡∏°! ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏ß‡πâ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡∏¢‡∏£‡∏ï‡∏¥‡∏ô‡∏¥‡∏¢‡∏°‡∏≠‡∏¢‡∏π‡πà‡πÑ‡∏°‡πà‡πÑ‡∏Å‡∏•")

    st.markdown("---")
    st.info("üí° ‡∏ô‡πâ‡∏≠‡∏á‡πÜ ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ñ‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏ï‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏û‡∏µ‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞!")

# --- 5. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Chat History ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if os.path.exists("ku_data.txt"):
    with open("ku_data.txt", "r", encoding="utf-8") as f:
        knowledge_base = f.read()
else:
    knowledge_base = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏Ç‡∏ï‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤"

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
for message in st.session_state.messages:
    avatar = "üßë‚Äçüéì" if message["role"] == "user" else "üêØ"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# --- 6. ‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á ChatBot ---
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà... (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡∏≠‡πÉ‡∏ö KU3 ‡∏´‡∏£‡∏∑‡∏≠ ‡∏ï‡∏∂‡∏Å 17 ‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡πÑ‡∏´‡∏ô?)"):
    st.chat_message("user", avatar="üßë‚Äçüéì").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant", avatar="üêØ"):
        placeholder = st.empty()
        placeholder.markdown("*(‡∏û‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ô‡∏∞...)*")
        
        history = [{"role": "user" if m["role"] == "user" else "model", "parts": [m["content"]]} 
                   for m in st.session_state.messages[-6:-1]]
        
        try:
            chat_session = model.start_chat(history=history)
            full_context = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{knowledge_base}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {prompt}"
            
            response = chat_session.send_message(full_context, stream=True)
            full_response = ""
            for chunk in response:
                full_response += chunk.text
                placeholder.markdown(full_response + "‚ñå")
            
            placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            st.rerun()
            
        except Exception as e:
            st.error(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
