import streamlit as st
import google.generativeai as genai
import os

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ ---
st.set_page_config(page_title="‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ - KU Sriracha Bot", page_icon="üêØ", layout="wide")

# --- CSS ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° ---
st.markdown("""
<style>
    .stApp { background-color: #FFFFFF; color: black; }
    [data-testid="stSidebar"] { background-color: #00594C !important; }
    .stSidebar [data-testid="stMarkdownContainer"] p, .stSidebar h3 { color: white !important; }
    h1 { color: #00594C !important; }
    .stChatMessage { border-radius: 15px; margin-bottom: 10px; }
</style>
""", unsafe_allow_html=True)

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ API ---
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    st.error("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Streamlit Secrets")
    st.stop()

genai.configure(api_key=api_key)

@st.cache_resource
def load_model():
    instruction = (
        "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ '‡∏ô‡πâ‡∏≠‡∏á‡∏ô‡∏ô‡∏ó‡∏£‡∏µ' AI ‡∏£‡∏∏‡πà‡∏ô‡∏û‡∏µ‡πà ‡∏°‡∏Å. ‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤ (KU SRC) "
        "1. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á' ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å "
        "2. ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏° ‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏•‡∏¥‡∏á‡∏Å‡πå PDF ‡∏ï‡∏£‡∏á‡πÜ ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ "
        "3. ‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≤‡∏°‡∏´‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå Google Maps ‡∏ó‡∏µ‡πà‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ "
        "4. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏•‡∏∞‡∏ï‡∏¥‡∏à‡∏π‡∏î/‡∏•‡∏≠‡∏á‡∏à‡∏¥‡∏à‡∏π‡∏î (GPS) ‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏´‡πá‡∏ô "
        "5. ‡πÉ‡∏ä‡πâ‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° ‡∏û‡∏µ‡πà-‡∏ô‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏™‡∏∏‡∏†‡∏≤‡∏û"
    )
    return genai.GenerativeModel(model_name="gemini-1.5-flash", system_instruction=instruction)

model = load_model()

# --- ‡∏™‡πà‡∏ß‡∏ô Sidebar (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Iframe ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á) ---
with st.sidebar:
    st.image("https://www.src.ku.ac.th/th/images/logo/KU_Sriracha_Logo.png", width=150)
    st.markdown("### üìç ‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏Ç‡∏ï‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤")
    
    # ‡πÉ‡∏ä‡πâ‡∏û‡∏¥‡∏Å‡∏±‡∏î‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á ‡∏°‡∏Å.‡∏®‡∏£‡∏ä. ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á Error pb parameter
    map_url = "https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3882.443217424641!2d100.9201!3d13.1158!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x3102b1156a5c531d%3A0x600b2158864700!2z4Lih4Lir4Liy4Lin4Li04LiX4Lii4Liy4Lis4Liy4LiU4Lio4Liy4Liq4LiV4Lij4LiU4LmA4LiB4Liy4Lij4LiK4Liy4Liq4Liy4LiX4Lij4Liy4LiE4Li1!5e0!3m2!1sth!2sth!4v1700000000000"
    st.components.v1.html(f'<iframe src="{map_url}" width="100%" height="300" style="border:0; border-radius:10px;" allowfullscreen="" loading="lazy"></iframe>', height=320)
    st.info("üí° ‡∏ô‡πâ‡∏≠‡∏á‡πÜ ‡∏ñ‡∏≤‡∏°‡∏ó‡∏≤‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≠‡πÅ‡∏ö‡∏ö‡∏ü‡∏≠‡∏£‡πå‡∏°‡∏Å‡∏±‡∏ö‡∏û‡∏µ‡πà‡∏ô‡∏ô‡∏ó‡∏£‡∏µ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö")

# --- ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# ‡πÇ‡∏´‡∏•‡∏î Knowledge Base
if os.path.exists("ku_data.txt"):
    with open("ku_data.txt", "r", encoding="utf-8") as f:
        knowledge_base = f.read()
else:
    knowledge_base = "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏°‡∏Å. ‡∏®‡∏£‡∏µ‡∏£‡∏≤‡∏ä‡∏≤"

# ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
for m in st.session_state.messages:
    with st.chat_message(m["role"], avatar="üßë‚Äçüéì" if m["role"] == "user" else "üêØ"):
        st.markdown(m["content"])

# ‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏±‡∏ö Input
if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..."):
    st.chat_message("user", avatar="üßë‚Äçüéì").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant", avatar="üêØ"):
        placeholder = st.empty()
        placeholder.markdown("*(‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•...)*")
        
        full_prompt = f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:\n{knowledge_base}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {prompt}"
        
        try:
            # ‡πÉ‡∏ä‡πâ Stream ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏î‡∏π‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå
            response = model.generate_content(full_prompt, stream=True)
            full_response = ""
            for chunk in response:
                full_response += chunk.text
                placeholder.markdown(full_response + "‚ñå")
            
            placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            st.rerun()
        except Exception as e:
            st.error(f"Error: {str(e)}")
